

## LangChain 기반 Pinecone RAG 시스템 정리

음식 이미지로부터 요리의 풍미를 요약하고, 해당 풍미에 어울리는 와인을 **LangChain + Pinecone** 기반 RAG로 추천하는 체인 구조.


```
[1] CSV 문서 로딩 → [2] 임베딩 → [3] Pinecone 벡터 저장
       ↓
[4] 음식 풍미 요약 (LLM)
       ↓
[5] 풍미 기반 유사 와인 검색 (Pinecone)
       ↓
[6] 풍미 + 리뷰 기반 와인 추천 (LLM)
```


## 1. CSV 문서 로딩 – `CSVLoader`

> CSV 기반 와인 리뷰 데이터를 LangChain의 문서 형식으로 로딩

```python
from langchain_community.document_loaders import CSVLoader

loader = CSVLoader("./winemag-data-130k-v2.csv", encoding="utf-8")
documents = loader.load()
```

* 반환된 `documents`는 LangChain 문서 객체 리스트이며, 각 문서는 `.page_content`와 `.metadata`를 가짐


## 2. 임베딩 – `OpenAIEmbeddings`

> 문서를 의미 기반 벡터로 변환해 Pinecone에 저장할 수 있도록 준비

```python
from langchain_openai.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
```

* 이 임베딩은 `embed_query` 또는 `embed_documents`로 문장을 벡터로 변환


## 3. Pinecone 벡터 저장소 생성 – `PineconeVectorStore`

> 문서 임베딩을 벡터 DB(Pinecone)에 저장하거나, 저장된 DB에서 유사 문서를 검색

```python
from langchain_pinecone import PineconeVectorStore
import os

vector_db = PineconeVectorStore.from_documents(
    documents,
    embeddings,
    index_name=os.getenv("PINECONE_INDEX_NAME"),
    namespace=os.getenv("PINECONE_NAMESPACE"),
    pinecone_api_key=os.getenv("PINECONE_API_KEY")
)
```

* `.from_documents()`로 최초 삽입
* 이후 검색 시에는 `similarity_search()` 활용


## 4. 음식 풍미 요약 – `describe_dish_flavor()`

> 음식 이미지를 기반으로 GPT 모델이 요리 이름과 풍미를 한 문장으로 요약

```python
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

def describe_dish_flavor(query):
    prompt = ChatPromptTemplate([
        ("system", """
        Persona: 당신은 풍미 전문가이며 음식에 대한 깊은 이해를 바탕으로 요리의 맛을 분석합니다...
        """),
        ("user", "이미지의 요리명과 풍미를 한 문장으로 요약해주세요.")
    ])
    
    prompt += HumanMessagePromptTemplate.from_template([query])  # 이미지 입력
    model = ChatOpenAI(model_name="gpt-4.1", temperature=1)
    return prompt | model | StrOutputParser()
```

* 이미지에 대한 질문 템플릿을 구성하고, LLM이 그에 대해 응답
* 최종 결과는 요리명과 풍미 요약 문장


## 5. 풍미 기반 유사 와인 검색 – `search_wines()`

> 요리 풍미를 쿼리로 사용하여 Pinecone에서 유사 와인 리뷰 5개 검색

```python
def search_wines(query):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_db = PineconeVectorStore(
        embedding=embeddings,
        index_name=PINECONE_INDEX_NAME,
        namespace=PINECONE_NAMESPACE,
        pinecone_api_key=PINECONE_API_KEY
    )
    results = vector_db.similarity_search(query, k=5)
    
    return {
        "dish_flavor": query,
        "wine_reviews": "\n".join([doc.page_content for doc in results])
    }
```

* 풍미 요약을 쿼리로 사용
* 유사한 와인 리뷰들을 연결된 `page_content` 기반으로 가공하여 반환


## 6. 와인 추천 – `recommend_wines()`

> 풍미 + 유사 와인 리뷰를 기반으로 GPT가 최종 와인을 추천

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage

def recommend_wines(query):
    prompt = ChatPromptTemplate([
        ("system", """Persona: 당신은 와인 전문가이며 요리와 와인 페어링에 능숙합니다..."""),
        HumanMessagePromptTemplate.from_template("""
        와인 페어링 추천에 아래의 요리와 풍미, 와인 리뷰만을 참고하여 한글로 답변해주세요.
        요리와 풍미:
        {dish_flavor}
        와인 리뷰:
        {wine_reviews}
        """)
    ])
    
    model = ChatOpenAI(model_name="gpt-4.1", temperature=1)
    return prompt | model | StrOutputParser()
```

* 최종 응답은 한글로 작성된 페어링 추천 문장


## 7. 전체 체인 구성 – `RunnableLambda`

> 위 3개 함수를 LangChain Runnable 체인으로 연결

```python
from langchain_core.runnables import RunnableLambda

# 단일 단계 실행
RunnableLambda(describe_dish_flavor).invoke({"image_url": "..."})

# 전체 체인 연결
chain = RunnableLambda(describe_dish_flavor) | RunnableLambda(search_wines) | RunnableLambda(recommend_wines)

result = chain.invoke({"image_url": "..."})
print(result)
```

* 이미지 URL을 입력하면 음식 분석 → 유사 리뷰 검색 → 와인 추천까지 자동 수행
