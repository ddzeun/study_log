
# LangChain

## 1. PromptTemplate

> **단순한 텍스트 기반 프롬프트가 필요한 경우 사용**

LangChain에서 가장 기본이 되는 프롬프트 구성 방식. 문자열 템플릿에 변수를 넣어 사용자가 입력한 값에 따라 동적으로 문장을 생성할 수 있음

```python
from langchain import PromptTemplate

template = "{product}를 홍보하기 위한 재미있고, 새로운 광고문구를 작성해주세요."
prompt = PromptTemplate(template=template, input_variables=["product"])
prompt.format(product="카메라")
```

## 2. ChatPromptTemplate

> **Chat 모델 (GPT 등)과 대화 흐름을 구성할 때 사용**

시스템 메시지, 사용자 메시지, AI 응답 등을 구분해 메시지 흐름을 구성할 수 있다. OpenAI의 Chat API 등 대화형 모델에 적합한 형태.

```python
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

system = SystemMessagePromptTemplate.from_template("당신은 도움을 주는 챗봇입니다.")
human = HumanMessagePromptTemplate.from_template("질문: {question}")

messages = ChatPromptTemplate.from_messages([system, human])
prompt = messages.format_messages(question="AI가 무엇인가요?")
```

| 역할     | 설명                                        |
| ------ | ----------------------------------------- |
| System | 모델의 역할을 정의함 (`Assistant`, `Translator` 등) |
| Human  | 사용자의 입력을 전달                               |
| AI     | 모델의 이전 응답을 포함할 수도 있음 (few-shot 목적)        |

## 3. FewShotPromptTemplate

> **예시를 기반으로 모델에게 패턴을 학습시키고 싶을 때 사용**

하나의 작업을 설명하는 대신, 여러 개의 문제-답 예시를 보여줘서 모델이 패턴을 추론하게 함. 예시 기반 학습이 필요한 경우 유용.

```python
from langchain.prompts import FewShotPromptTemplate, PromptTemplate

examples = [
    {"question": "2 + 2는 무엇인가요?", "answer": "2 + 2 = 4"},
    {"question": "3 + 5는 무엇인가요?", "answer": "3 + 5 = 8"},
]

example_prompt = PromptTemplate(
    template="Q: {question}\nA: {answer}",
    input_variables=["question", "answer"]
)

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="다음 계산 문제를 해결하세요.",
    suffix="Q: {question}\nA:",
    input_variables=["question"]
)
```

## 4. Output Parser – `CommaSeparatedListOutputParser`

> **모델 출력 형식을 강제하고 파싱하고 싶을 때 사용**

모델 응답을 구조화된 형태로 정리하려면 파서가 필요하다. 예: 리스트, 딕셔너리, JSON 등 원하는 형식으로 유도하고 `.parse()`로 파싱 가능.

```python
from langchain.output_parsers import CommaSeparatedListOutputParser

parser = CommaSeparatedListOutputParser()
format_instructions = parser.get_format_instructions()
```

```python
from langchain import PromptTemplate

prompt_template = PromptTemplate(
    template="{subject} 10개의 팀을 보여주세요.\n형식지정: {format_instructions}",
    input_variables=["subject"],
    partial_variables={"format_instructions": format_instructions}
)
```

## 5. Model 호출 - `ChatOpenAI`

> **OpenAI의 Chat 모델을 LangChain과 연동할 때 사용**

GPT 계열 모델을 LangChain과 연결해 프롬프트를 전달하고 응답을 받을 수 있다. `.invoke()`로 호출, `.parse()`로 응답 구조화.

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o",
    temperature=1,
    max_tokens=2048
)

response = model.invoke(prompt)
parsed_response = output_parser.parse(response.content)
```

## 6. HuggingFace 모델 연동

> **HuggingFace의 LLM을 LangChain과 함께 사용할 때**

HuggingFace의 사전 학습 모델도 LangChain에서 연동 가능. `HuggingFaceEndpoint`로 모델 설정, `ChatHuggingFace`로 감싸 채팅형 인터페이스 제공.

```python
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

endpoint = HuggingFaceEndpoint(
    repo_id="mistralai/Mixtral-8x7B-Instruct-v0.1",
    task="text-generation",
    max_new_tokens=4096
)

model2 = ChatHuggingFace(llm=endpoint)
model2.invoke("은정이는 강아지를 키우고 있습니다.")
```

## 7. ModelLaboratory

> **여러 모델의 응답을 비교하고 실험할 때 사용**

동일한 질문에 대해 여러 모델이 어떻게 응답하는지 실험할 수 있다. 모델의 응답 품질, 톤, 속도 등을 비교하고 싶을 때 유용.

```python
from langchain.model_laboratory import ModelLaboratory

model_lab = ModelLaboratory.from_llms([model1, model2])
model_lab.compare("대한민국의 여름은 몇 월부터 몇 월인가요?")
```


# LangChain Retrieval

## Retrieval이란?

> **모델이 알지 못하는 정보를 외부 문서에서 검색하여 질문에 답하게 하는 방식**

기존 LLM은 훈련 범위를 벗어난 최신 정보에 대해 정확한 응답이 어렵다. 이를 해결하기 위해, 외부 문서 → 벡터 변환 → 유사 문서 검색 → LLM 응답 방식이 사용.

### 전체 구조

```
[문서 로딩] → [임베딩] → [벡터 저장] → [유사도 검색] → [LLM 응답 생성]
```

## 1. Document Loaders

문서를 LangChain에서 사용할 수 있는 형식으로 로딩하는 도구. 다양한 포맷(PDF, 웹, 텍스트 등)을 지원함.

### WebBaseLoader

```python
from langchain_community.document_loaders import WebBaseLoader

url = "https://ko.wikipedia.org/wiki/대왕판다"
loader = WebBaseLoader(url)
documents = loader.load()
```

### PyPDFLoader

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("The_Adventures_of_Tom_Sawyer.pdf")
documents = loader.load()
```

## 2. Embedding

> **문장을 의미 기반 벡터로 변환하여 검색 가능하게 만드는 단계**

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vector = embeddings.embed_query("문장 예시")
vectors = embeddings.embed_documents([doc.page_content for doc in documents])
```

## 3. 벡터 저장소 - FAISS

> **임베딩된 문서를 저장하고 유사도 검색 수행**

```python
from langchain.vectorstores import FAISS

vector_store = FAISS.from_documents(documents, embeddings)
vector_store.similarity_search("톰소여", k=3)
```

## 4. Retriever 객체 생성

> **벡터 저장소에 대한 검색 기능을 래핑한 객체**

```python
retriever = vector_store.as_retriever()
```

## 5. RetrievalQA 체인 구성

> **문서 기반 질문 응답 기능을 LLM에 연결**

`chain_type="stuff"`: 검색된 문서를 그대로 LLM에 연결하여 응답 생성

```python
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o", temperature=0)

retriever_qa = RetrievalQA.from_chain_type(
    llm=model,
    retriever=retriever,
    chain_type="stuff"
)
```

### 전체 흐름 정리

```
[1] 문서 로딩
    - WebBaseLoader / PyPDFLoader
       ↓
[2] 임베딩
    - OpenAIEmbeddings
       ↓
[3] 벡터 DB 저장
    - FAISS
       ↓
[4] 검색기 생성
    - Retriever
       ↓
[5] RetrievalQA 체인 구성
       ↓
[6] 질문 → 유사 문서 추출 → 응답 생성
```
